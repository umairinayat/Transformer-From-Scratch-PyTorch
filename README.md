# Transformer-From-Scratch-PyTorch
This repository contains a PyTorch implementation of a Transformer model built from scratch. It includes components such as Scaled Dot-Product Attention, Multi-Head Attention, Positional Encoding, and a full Encoder-Decoder architecture. The repository is designed for educational purposes, showcasing how Transformers work.
